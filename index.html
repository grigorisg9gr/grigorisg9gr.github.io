---
layout: single
title: About me
author_profile: true
tags: [About me, machine learning]
comments: false
---


<p>Welcome to my site. My name is Grigoris and I am an Assistant Professor in University of Wisconsin-Madison. </p>

<p>My research focuses on reliable machine learning. Concretely,<p>

<ul>
    <li><strong>Parsimonious learning</strong>: How do deep neural networks learn so effectively using natural signal priors? Natural signals like text and images aren't random; they have a hidden structure, often in the form of sparsity or low-rank constraints. Understanding precisely how networks leverage this inherent structure is a key question. My goal is to design the next generation of networks that have more predictable expressivity (what functions they can represent), trainability (how optimization finds a good solution), and generalization (why they perform well on unseen data).</li>

    <li><strong>Trustworthy models</strong>: How can we build generative models that we can truly trust? Despite their impressive capabilities, existing models can be surprisingly fragile to adversarial attacks, to reliable extrapolations, and can generate nonsensical outputs (hallucinations). My goal is to design the next generation of trustworthy models with predictable and reliable behavior. </li>
</ul>

<h2>News</h2>
<ul>
    <li>August 2025: The slides for my talks at MLSS on <a href="https://uwmadison.box.com/s/1btp95pw4m3two59xifkja6zub8w39dc">adversarial attacks</a> and <a href="https://uwmadison.box.com/s/wgm2ui20q7b9gdibwmw0qs2fgggi60id">trustworthy ML</a>. Thanks Pablo for organizing <a href="https://eventosdiee.ucsp.edu.pe/mlss/">MLSS</a>.</li>
    <li>July 2025: Invited talks on the premier Machine Learning Summer School (MLSS) at <a href="https://eventosdiee.ucsp.edu.pe/mlss/">Universidad Cat√≥lica San Pablo</a>. Thank for the invitation Pablo, Mariano and Efrain.</li>
    <li>May 2025: The following paper is accepted at <strong>ICML 2025</strong>: <a href="https://arxiv.org/abs/2410.05603">Everything Everywhere All at Once: LLMs can In-Context Learn Multiple Tasks in Superposition</a>.</li>
    <li>April 2025: I participated in the NSF-funded workshop on '<a href="https://cse.buffalo.edu/~jsyuan/ResponsibleAI_NSF.html">US-Southeast Asia Regional Workshop on Responsible Artificial Intelligence</a>'.</li>
    <li>April 2025: The following paper has been accepted at <strong>Transactions on Pattern Analysis and Machine Intelligence</strong>: '<em><a href="https://ieeexplore.ieee.org/document/10965485">Hadamard product in deep learning: Introduction, Advances and Challenges</a></em>'. This survey summarizes many of the insights obtained the last few years using the Hadamard product.</li>
    <li>April 2025: We are organizing a Dagstuhl seminar on "<a href='https://www.dagstuhl.de/26102'>Tensor Factorizations Meet Probabilistic Circuits</a>". More information soon.</li>
    <li>April 2025: Invited talk on the workshop "<a href='https://simons.berkeley.edu/workshops/future-language-models-transformers'>The Future of Language Models and Transformers</a>" in the prestigious <strong><a href='https://simons.berkeley.edu/homepage'>Simons Institute for the Theory of Computing</a></strong> in Berkeley. Thanks Sasha and Swabha for organizing the workshop. The talk is available <a href='https://www.youtube.com/watch?v=mod3sWAWX-U'>here</a>, while the slides are uploaded <a href="https://uwmadison.box.com/s/3ju7ilazj3zdmofxfztwovxt6jhx46t1">here</a>.</li>
    <li>March 2025: I interacted with <strong>Bloomberg</strong> for their coverage on <a href="https://www.bloomberg.com/news/newsletters/2025-03-27/an-old-approach-to-ai-gains-new-attention-after-deepseek">Mixture-of-Experts</a> mechanism.</li>
    <li>March 2025: <strong><a href="https://cpal.cc/rising_stars_presentations/">CPAL</a> Rising Star Award</strong>: Conference on Parsimony and Learning (CPAL) 2025.</li>
    <li>March 2025: I was selected as a new <a href="https://dsi.wisc.edu/about/dsi-affiliates/">Data Science Institute Affiliate</a>.</li>
    <li>February 2025: The following paper is accepted at <strong>ICLR 2025</strong>: <a href="https://arxiv.org/abs/2501.13676">Certified Robustness Under Bounded Levenshtein Distance</a>.</li>
    <li>January 2025: We are organizing a workshop titled "<a href='https://ml-theoretical-foundations-to-practice.github.io/'>Another Brick in the AI Wall: Building Practical Solutions from Theoretical Foundations</a>" in conjunction with CVPR'25.</li>
    <li>December 2024: We are organizing a workshop titled "<a href='https://uncertainty-foundation-models.github.io/'>Quantify Uncertainty and Hallucination in Foundation Models: The Next Frontier in Reliable AI</a>" in conjunction with ICLR'25.</li>
    <li>October 2024: We are organizing a workshop titled '<a href='https://april-tools.github.io/colorai/'>ColorAI: Connecting Low-Rank Representations in AI</a>' in conjunction with AAAI'25.</li>
    <li>September 2024: The following paper is accepted at <strong>NeurIPS 2024</strong>: <a href="https://arxiv.org/abs/2402.12550">Multilinear Mixture of Experts: Scalable Expert Specialization through Factorization</a>.</li>
    <li>September 2024: I am delivering a tutorial titled 'Architecture design: from neural networks to foundation models' in conjunction with <a href="https://dsaa2024.dsaa.co">DSAA 2024</a> on 8th October. Addition info on the <a href="https://grigoris.ece.wisc.edu/tutorials/2024_DSAA/">tutorial site</a>. </li>
    <li>July 2024: We are organizing a workshop titled '<a href="https://sites.google.com/view/neurips2024-ftw/home">Fine-Tuning in Modern Machine Learning: Principles and Scalability</a>' in conjunction with NeurIPS'24.</li>
    <li>July 2024: The following paper is accepted at <strong>ECCV 2024</strong>: <a href="https://arxiv.org/abs/2407.07284">Multi-Identity Gaussian Splatting via Tensor Decomposition</a>.</li>
    <li>July 2024: We are delivering a tutorial titled 'Scaling and Reliability Foundations in Machine Learning' in conjunction with <a href="https://2024.ieee-isit.org/home">ISIT 2024</a> on 7th July. </li>
    <li>June 2024: Grateful to Google and OpenAI for their grants supporting our research on trustworthy Large Language Models (LLMs).</li>
    <li>May 2024: The following papers are accepted at <strong>ICML 2024</strong>: 
        <ul>
                <li>'<span style="color: #0000ff;"><a href="https://openreview.net/pdf/3ca014f375f8aa0572aad3f58b3d89bc2a2f80d7.pdf">Revisiting character-level adversarial attacks for Language Models</a></span>',</li>
                <li>'<span style="color: #0000ff;"><a href="https://openreview.net/pdf?id=k10805cgak">Learning to Remove Cuts in Integer Linear Programming</a></span>',</li>
                <li>'<span style="color: #0000ff;"><a href="https://openreview.net/pdf/670b48719735353ec2924a4ef3d0a1cdb3d30749.pdf">Going beyond compositional generalization, DDPM can produce zero-shot interpolation</a></span>',</li>
                <li>'<span style="color: #0000ff;"><a href="https://openreview.net/pdf?id=9GbAea74O6">REST: Efficient and Accelerated EEG Seizure Analysis through Residual State Updates</a></span>'.</li>
        </ul></li>    
    <li>April 2024: We are organizing a tutorial titled 'Scaling and Reliability Foundations in Machine Learning' in conjunction with <a href="https://2024.ieee-isit.org/home">ISIT 2024</a> on 7-th July. </li>
    <li>January 2024: The following papers are accepted at <strong>ICLR 2024</strong>:
        <ul>
                <li>'<span style="color: #0000ff;"><a href="https://arxiv.org/abs/2403.09889">Generalization of Scaled Deep ResNets in the Mean-Field Regime</a></span>' (as spotlight),</li>
                <li>'<span style="color: #0000ff;"><a href="https://arxiv.org/abs/2401.17992">Multilinear Operator Networks</a></span>',</li>
                <li>'<span style="color: #0000ff;"><a href="https://arxiv.org/abs/2401.11618">Efficient local linearity regularization to overcome catastrophic overfitting</a></span>',</li>
                <li>'<span style="color: #0000ff;"><a href="https://arxiv.org/abs/2403.13134">Robust NAS under adversarial training: benchmark, theory, and beyond</a></span>'.</li>
        </ul></li>
    <li>November 2023: I was recognized as a <a href="https://neurips.cc/Conferences/2023/ProgramCommittee"><strong>top reviewer</strong></a> at <strong>NeurIPS 2023</strong>.</li>
    <li>October 2023: The following papers are accepted at <strong>NeurIPS 2023</strong>: <span style="color: #0000ff;"><a href="https://arxiv.org/abs/2310.18672">`Maximum Independent Set: Self-Training through Dynamic Programming'</a></span> and <span style="color: #0000ff;"><a href="https://arxiv.org/abs/2311.01575">`On the Convergence of Encoder-Only Shallow Transformers'</a></span>.
    <li>June 2023: The slides and the recording of our tutorial titled `Deep Learning Theory for Vision' at CVPR'23 are available: <a href="https://dl-theory.github.io/assets/CVPR.pdf">Slides</a> and <a href="https://www.youtube.com/watch?v=XtPGP9SXyiI">recording</a>. More information: <a href="https://dl-theory.github.io/">https://dl-theory.github.io/</a>. 
    <li>May 2023: The following paper has been accepted at <strong>Transactions on Machine Learning Research (TMLR)</strong>: <a href="https://openreview.net/forum?id=N7lCDaeNiS">`Federated Learning under Covariate Shifts with Generalization Guarantees'</a>.
    <li>April 2023: The following paper has been accepted at <strong>ICML 2023</strong>: <a href="https://arxiv.org/abs/2305.19377">`Benign Overfitting in Deep Neural Networks under Lazy Training'</a>.
    <li>April 2023: Awarded the <a href="https://www.daad.de/en/the-daad/postdocnet/details-and-application/">DAAD AInet Fellowship</a>, which is awarded to outstanding early career researchers. Topic: generative models in ML.</li>
    <li>March 2023: The following paper has been accepted at <strong>CVPR 2023</strong>: '<em><a href="http://arxiv.org/abs/2303.13896">Regularization of polynomial networks for image recognition</a></em>'. </li>
    <li>February 2023: Organizer of the tutorial on 'Polynomial Nets' in conjunction with AAAI'23: <a href="https://polynomial-nets.github.io/">https://polynomial-nets.github.io/</a>.
    <li>January 2023: The following paper has been accepted at <strong>Transactions on Machine Learning Research (TMLR)</strong>: '<em><a href="https://openreview.net/forum?id=wkecshlYxI">Revisiting adversarial training for the worst-performing class</a></em>'. </li>
    <li>December 2022: The following paper has been accepted at <strong>Transactions on Pattern Analysis and Machine Intelligence</strong>: '<em><a href="https://ieeexplore.ieee.org/abstract/document/10076897">Linear Complexity Self-Attention with 3rd Order Polynomials</a></em>'. </li>
    <li>October 2022: I was recognized as a <a href="https://neurips.cc/Conferences/2022/ProgramCommittee"><strong>best reviewer</strong></a> at <strong>NeurIPS 2022</strong>.</li>
    <li>September 2022: The following papers have been accepted at <strong>NeurIPS 2022</strong>: 
        <ul>
                <li>'<span style="color: #0000ff;"><a href="https://openreview.net/pdf?id=m8vzptcFKsT">Robustness in deep learning: The good (width), the bad (depth), and the ugly (initialization)</a></span>',</li>
                <li>'<span style="color: #0000ff;"><a href="https://openreview.net/pdf?id=aQySSrCbBul">Generalization Properties of NAS under Activation and Skip Connection Search</a></span>',</li>
                <li>'<span style="color: #0000ff;"><a href="https://openreview.net/pdf?id=gsdHDI-p6NI">Sound and Complete Verification of Polynomial Networks</a></span>',&nbsp;</li>
                <li>'<span style="color: #0000ff;"><a href="https://openreview.net/pdf?id=_cXUMAnWJJj">Extrapolation and Spectral Bias of Neural Nets with Hadamard Product: a Polynomial Net Study</a></span>'.</li>
        </ul></li>
    <li>August 2022: <a href="https://www.slideshare.net/GrigorisChrysos/tutorial-on-polynomial-networks-at-cvpr22">The slides</a> used in the tutorial on polynomial networks (organized at CVPR'22) have been released. </li>
    <li>July 2022: I was awarded a <a href="https://icml.cc/Conferences/2022/Reviewers"><strong>best reviewer award (top 10%)</strong></a> at <strong>ICML 2022</strong>.</li>
    <li>July 2022: The following papers have been accepted at <strong>ECCV 2022</strong>: '<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136850682.pdf">Augmenting Deep Classifiers with Polynomial Neural Networks</a>' and '<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680457.pdf">MimicME: A Large Scale Diverse 4D Database for Facial Expression Analysis</a>'. More information soon.</li>
    <li>June 2022: Organizer of the tutorial on 'Polynomial Nets' in conjunction with CVPR'22: <a href="https://polynomial-nets.github.io/">https://polynomial-nets.github.io/previous_versions/index.html</a>.
    <li>April 2022: I was awarded a <a href="https://iclr.cc/Conferences/2022/Reviewers"><strong>highlighted reviewer award</strong></a> at <strong>ICLR 2022</strong>.</li>
    <li>March 2022: The following paper has been accepted at <strong>CVPR 2022</strong>: '<span style="color: #0000ff;"><em><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Georgopoulos_Cluster-Guided_Image_Synthesis_With_Unconditional_Models_CVPR_2022_paper.pdf">Cluster-guided Image Synthesis with Unconditional Models</a></em></span>'.</li>
    <li>February 2022: <a href="https://www.youtube.com/watch?v=l1xievUFdAw"><strong>My talk</strong></a> on polynomial networks at the  UCL Centre for Artificial Intelligence has been uploaded online.</li>
    <li>January 2022: The following papers have been accepted at <strong>ICLR 2022</strong>: '<span style="color: #0000ff;"><em><a href="https://openreview.net/pdf?id=dQ7Cy_ndl1s">Controlling the Complexity and Lipschitz Constant improves Polynomial Nets</a></em></span>' and '<span style="color: #0000ff;"><em><a href="https://openreview.net/pdf?id=P7FLfMLTSEX">The Spectral Bias of Polynomial Neural Networks</a></em></span>'.</li>
</ul>


<h2>Funding Acknowledgement</h2>

I would like to acknowledge the funding of the following organizations who have generously supported various events or projects in the past. I am very thankful for their support:
<ul>
    <li>2025: Zulip: Sponsored hosting from <a href="https://zulip.com/">Zulip</a>, which is an open-source team collaboration tool.</li>
    <li>2024: Google and OpenAI: grants on trustworthy Large Language Models (LLMs).</li>
    <li>2024: ELISE Fellows Mobility Program: travel grant for short-term visit of an ELLIS lab.</li>
</ul>
