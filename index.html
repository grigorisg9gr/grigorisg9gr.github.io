---
layout: single
title: About me
author_profile: true
tags: [About me, machine learning]
comments: false
---


<p>Welcome to my site. My name is Grigoris and I am an Assistant Professor in University of Wisconsin-Madison. </p>

<p>My research focuses on reliable machine learning, focusing on the development of robust models that perform well under noise and out-of-distribution data. Concretely,<p>

<ul>
    <li><strong>Architecture design</strong>: I have worked extensively on polynomial networks (PNs) that capture high-degree interactions between inputs. My short-term goals are to understand the inductive bias and properties of existing architectures through empirical and theoretical studies. I am interested in the complete theoretical understanding of (neural/polynomial) networks, including their expressivity, trainability, generalization properties, and inductive biases. Our recent work has provided the first characterization of the generalization of this class of functions, or the spectral bias of high-degree polynomials, highlighting how PNs can learn higher frequency functions faster than regular feed-forward networks.</li>

    <li><strong>Trustworthy models</strong>: My goal is to understand the enhance the performance of existing networks, particularly with respect to their extrapolation abilities, and their robustness to malicious (adversarial) attacks. I am interested in both discriminative and generative models, including Large Language Models. In our recent work, we have studied adversarial attacks and defenses in the text domain, where there are exciting questions ahead. Our long-term goal is to develop models that are robust, fair, and capable of generalizing well to unseen combinations with strong extrapolation abilities. </li>
</ul>

<h2>News</h2>
<ul>
    <li>March 2025: <a href="https://cpal.cc/rising_stars_presentations/">CPAL</a> Rising Star Award: Conference on Parsimony and Learning (CPAL) 2025. 
    <li>March 2025: I was selected as a new <a href="https://dsi.wisc.edu/about/dsi-affiliates/">Data Science Institute Affiliate</a>.</li>
    <li>February 2025: The following paper is accepted at <strong>ICLR 2025</strong>: <a href="https://arxiv.org/abs/2501.13676">Certified Robustness Under Bounded Levenshtein Distance</a>.</li>
    <li>January 2025: We are organizing a workshop titled "<a href='https://ml-theoretical-foundations-to-practice.github.io/'>Another Brick in the AI Wall: Building Practical Solutions from Theoretical Foundations</a>" in conjunction with CVPR'25. <span style="color: #ff0000;">Submission portal until 15th March; looking forward to your submissions.</span></li>
    <li>December 2024: We are organizing a workshop titled "<a href='https://uncertainty-foundation-models.github.io/'>Quantify Uncertainty and Hallucination in Foundation Models: The Next Frontier in Reliable AI</a>" in conjunction with ICLR'25.</li>
    <li>October 2024: We are organizing a workshop titled '<a href='https://april-tools.github.io/colorai/'>ColorAI: Connecting Low-Rank Representations in AI</a>' in conjunction with AAAI'25.</li>
    <li>September 2024: The following paper is accepted at <strong>NeurIPS 2024</strong>: <a href="https://arxiv.org/abs/2402.12550">Multilinear Mixture of Experts: Scalable Expert Specialization through Factorization</a>.</li>
    <li>September 2024: I am delivering a tutorial titled 'Architecture design: from neural networks to foundation models' in conjunction with <a href="https://dsaa2024.dsaa.co">DSAA 2024</a> on 8th October. Addition info on the <a href="https://grigoris.ece.wisc.edu/tutorials/2024_DSAA/">tutorial site</a>. </li>
    <li>July 2024: We are organizing a workshop titled '<a href="https://sites.google.com/view/neurips2024-ftw/home">Fine-Tuning in Modern Machine Learning: Principles and Scalability</a>' in conjunction with NeurIPS'24.</li>
    <li>July 2024: The following paper is accepted at <strong>ECCV 2024</strong>: <a href="https://arxiv.org/abs/2407.07284">Multi-Identity Gaussian Splatting via Tensor Decomposition</a>.</li>
    <li>July 2024: We are delivering a tutorial titled 'Scaling and Reliability Foundations in Machine Learning' in conjunction with <a href="https://2024.ieee-isit.org/home">ISIT 2024</a> on 7th July. </li>
    <li>June 2024: Grateful to Google and OpenAI for their grants supporting our research on trustworthy Large Language Models (LLMs).</li>
    <li>May 2024: The following papers are accepted at <strong>ICML 2024</strong>: 
        <ul>
                <li>'<span style="color: #0000ff;"><a href="https://openreview.net/pdf/3ca014f375f8aa0572aad3f58b3d89bc2a2f80d7.pdf">Revisiting character-level adversarial attacks for Language Models</a></span>',</li>
                <li>'<span style="color: #0000ff;"><a href="https://openreview.net/pdf?id=k10805cgak">Learning to Remove Cuts in Integer Linear Programming</a></span>',</li>
                <li>'<span style="color: #0000ff;"><a href="https://openreview.net/pdf/670b48719735353ec2924a4ef3d0a1cdb3d30749.pdf">Going beyond compositional generalization, DDPM can produce zero-shot interpolation</a></span>',</li>
                <li>'<span style="color: #0000ff;"><a href="https://openreview.net/pdf?id=9GbAea74O6">REST: Efficient and Accelerated EEG Seizure Analysis through Residual State Updates</a></span>'.</li>
        </ul></li>    
    <li>April 2024: We are organizing a tutorial titled 'Scaling and Reliability Foundations in Machine Learning' in conjunction with <a href="https://2024.ieee-isit.org/home">ISIT 2024</a> on 7-th July. </li>
    <li>January 2024: The following papers are accepted at <strong>ICLR 2024</strong>:
        <ul>
                <li>'<span style="color: #0000ff;"><a href="https://arxiv.org/abs/2403.09889">Generalization of Scaled Deep ResNets in the Mean-Field Regime</a></span>' (as spotlight),</li>
                <li>'<span style="color: #0000ff;"><a href="https://arxiv.org/abs/2401.17992">Multilinear Operator Networks</a></span>',</li>
                <li>'<span style="color: #0000ff;"><a href="https://arxiv.org/abs/2401.11618">Efficient local linearity regularization to overcome catastrophic overfitting</a></span>',</li>
                <li>'<span style="color: #0000ff;"><a href="https://arxiv.org/abs/2403.13134">Robust NAS under adversarial training: benchmark, theory, and beyond</a></span>'.</li>
        </ul></li>
    <li>January 2024: The following paper has been accepted at <strong>Transactions on Machine Learning Research (TMLR)</strong>: '<em><a href="https://openreview.net/forum?id=oCBsxCov2g">PNeRV: A Polynomial Neural Representation for Videos</a></em>'. </li>
    <li>November 2023: I was recognized as a <a href="https://neurips.cc/Conferences/2023/ProgramCommittee"><strong>top reviewer</strong></a> at <strong>NeurIPS 2023</strong>.</li>
    <li>October 2023: The following papers are accepted at <strong>NeurIPS 2023</strong>: <span style="color: #0000ff;"><a href="https://arxiv.org/abs/2310.18672">`Maximum Independent Set: Self-Training through Dynamic Programming'</a></span> and <span style="color: #0000ff;"><a href="https://arxiv.org/abs/2311.01575">`On the Convergence of Encoder-Only Shallow Transformers'</a></span>.
    <li>June 2023: The slides and the recording of our tutorial titled `Deep Learning Theory for Vision' at CVPR'23 are available: <a href="https://dl-theory.github.io/assets/CVPR.pdf">Slides</a> and <a href="https://www.youtube.com/watch?v=XtPGP9SXyiI">recording</a>. More information: <a href="https://dl-theory.github.io/">https://dl-theory.github.io/</a>. 
    <li>May 2023: The following paper has been accepted at <strong>Transactions on Machine Learning Research (TMLR)</strong>: <a href="https://openreview.net/forum?id=N7lCDaeNiS">`Federated Learning under Covariate Shifts with Generalization Guarantees'</a>.
    <li>April 2023: The following paper has been accepted at <strong>ICML 2023</strong>: <a href="https://arxiv.org/abs/2305.19377">`Benign Overfitting in Deep Neural Networks under Lazy Training'</a>.
    <li>April 2023: Awarded the <a href="https://www.daad.de/en/the-daad/postdocnet/details-and-application/">DAAD AInet Fellowship</a>, which is awarded to outstanding early career researchers. Topic: generative models in ML.</li>
    <li>March 2023: The following paper has been accepted at <strong>CVPR 2023</strong>: '<em><a href="http://arxiv.org/abs/2303.13896">Regularization of polynomial networks for image recognition</a></em>'. </li>
    <li>February 2023: Organizer of the tutorial on 'Polynomial Nets' in conjunction with AAAI'23: <a href="https://polynomial-nets.github.io/">https://polynomial-nets.github.io/</a>.
    <li>January 2023: The following paper has been accepted at <strong>Transactions on Machine Learning Research (TMLR)</strong>: '<em><a href="https://openreview.net/forum?id=wkecshlYxI">Revisiting adversarial training for the worst-performing class</a></em>'. </li>
    <li>December 2022: The following paper has been accepted at <strong>Transactions on Pattern Analysis and Machine Intelligence</strong>: '<em><a href="https://ieeexplore.ieee.org/abstract/document/10076897">Linear Complexity Self-Attention with 3rd Order Polynomials</a></em>'. </li>
    <li>October 2022: I was recognized as a <a href="https://neurips.cc/Conferences/2022/ProgramCommittee"><strong>best reviewer</strong></a> at <strong>NeurIPS 2022</strong>.</li>
    <li>September 2022: The following papers have been accepted at <strong>NeurIPS 2022</strong>: 
        <ul>
                <li>'<span style="color: #0000ff;"><a href="https://openreview.net/pdf?id=m8vzptcFKsT">Robustness in deep learning: The good (width), the bad (depth), and the ugly (initialization)</a></span>',</li>
                <li>'<span style="color: #0000ff;"><a href="https://openreview.net/pdf?id=aQySSrCbBul">Generalization Properties of NAS under Activation and Skip Connection Search</a></span>',</li>
                <li>'<span style="color: #0000ff;"><a href="https://openreview.net/pdf?id=gsdHDI-p6NI">Sound and Complete Verification of Polynomial Networks</a></span>',&nbsp;</li>
                <li>'<span style="color: #0000ff;"><a href="https://openreview.net/pdf?id=_cXUMAnWJJj">Extrapolation and Spectral Bias of Neural Nets with Hadamard Product: a Polynomial Net Study</a></span>'.</li>
        </ul></li>
    <li>August 2022: <a href="https://www.slideshare.net/GrigorisChrysos/tutorial-on-polynomial-networks-at-cvpr22">The slides</a> used in the tutorial on polynomial networks (organized at CVPR'22) have been released. </li>
    <li>July 2022: I was awarded a <a href="https://icml.cc/Conferences/2022/Reviewers"><strong>best reviewer award (top 10%)</strong></a> at <strong>ICML 2022</strong>.</li>
    <li>July 2022: The following papers have been accepted at <strong>ECCV 2022</strong>: '<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136850682.pdf">Augmenting Deep Classifiers with Polynomial Neural Networks</a>' and '<a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136680457.pdf">MimicME: A Large Scale Diverse 4D Database for Facial Expression Analysis</a>'. More information soon.</li>
    <li>June 2022: Organizer of the tutorial on 'Polynomial Nets' in conjunction with CVPR'22: <a href="https://polynomial-nets.github.io/">https://polynomial-nets.github.io/previous_versions/index.html</a>.
    <li>April 2022: I was awarded a <a href="https://iclr.cc/Conferences/2022/Reviewers"><strong>highlighted reviewer award</strong></a> at <strong>ICLR 2022</strong>.</li>
    <li>March 2022: The following paper has been accepted at <strong>CVPR 2022</strong>: '<span style="color: #0000ff;"><em><a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Georgopoulos_Cluster-Guided_Image_Synthesis_With_Unconditional_Models_CVPR_2022_paper.pdf">Cluster-guided Image Synthesis with Unconditional Models</a></em></span>'.</li>
    <li>February 2022: <a href="https://www.youtube.com/watch?v=l1xievUFdAw"><strong>My talk</strong></a> on polynomial networks at the  UCL Centre for Artificial Intelligence has been uploaded online.</li>
    <li>January 2022: The following papers have been accepted at <strong>ICLR 2022</strong>: '<span style="color: #0000ff;"><em><a href="https://openreview.net/pdf?id=dQ7Cy_ndl1s">Controlling the Complexity and Lipschitz Constant improves Polynomial Nets</a></em></span>' and '<span style="color: #0000ff;"><em><a href="https://openreview.net/pdf?id=P7FLfMLTSEX">The Spectral Bias of Polynomial Neural Networks</a></em></span>'.</li>
</ul>


<h2>Funding Acknowledgement</h2>

I would like to acknowledge the funding of the following organizations who have generously supported various events or projects in the past. I am very thankful for their support:
<ul>
    <li>2024: Google and OpenAI: grants on trustworthy Large Language Models (LLMs).</li>
    <li>2024: ELISE Fellows Mobility Program: travel grant for short-term visit of an ELLIS lab.</li>
</ul>
